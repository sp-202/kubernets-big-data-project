apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupyterhub
  namespace: default
  labels:
    app: jupyterhub
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupyterhub
  template:
    metadata:
      labels:
        app: jupyterhub
    spec:
      serviceAccountName: jupyterhub
      containers:
        - name: jupyterhub
          image: subhodeep2022/spark-bigdata:jupyterhub-spark-connect-optimized-v2
          imagePullPolicy: Always
          ports:
            - containerPort: 8888
              name: web
          env:
            # Spark Connect Configuration
            - name: SPARK_REMOTE
              value: "sc://spark-connect-server-driver-svc:15002"
            - name: PYSPARK_PYTHON
              value: "python3"

            # S3/MinIO Configuration (Referencing global-config ConfigMap)
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: AWS_SECRET_ACCESS_KEY
            - name: JUPYTER_S3_ENDPOINT
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: MINIO_ENDPOINT
            - name: MINIO_ENDPOINT
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: MINIO_ENDPOINT

            # Jupyter Configuration
            - name: JUPYTER_ENABLE_LAB
              value: "yes"
            - name: JUPYTER_TOKEN
              value: "" # No token for dev (set for production)
            - name: GRANT_SUDO
              value: "yes"

          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"

          volumeMounts:
            # JupyterHub config (mounted to standard Docker Stacks location)
            - name: jupyter-config
              mountPath: /etc/jupyter/jupyter_notebook_config.py
              subPath: jupyter_notebook_config.py
            # Notebooks persistence (S3 sync handled by s3contents)
            - name: notebooks
              mountPath: /home/jovyan/work
            # Startup script for kernel setup
            - name: startup-script
              mountPath: /usr/local/bin/before-notebook.d/setup-kernels.sh
              subPath: setup-kernels.sh

      volumes:
        - name: jupyter-config
          configMap:
            name: jupyterhub-config
        - name: notebooks
          emptyDir: {} # Use PVC or S3 sync for persistence
        - name: startup-script
          configMap:
            name: jupyterhub-config
            defaultMode: 0755
---
# Service for JupyterHub Web UI
apiVersion: v1
kind: Service
metadata:
  name: jupyterhub
  namespace: default
spec:
  selector:
    app: jupyterhub
  ports:
    - protocol: TCP
      port: 8888
      targetPort: 8888
      name: web
  type: ClusterIP
---
# ServiceAccount for JupyterHub
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jupyterhub
  namespace: default
---
# ClusterRoleBinding for basic permissions (minimal, no executor management needed)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jupyterhub-spark
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view # Read-only access, no pod creation needed
subjects:
  - kind: ServiceAccount
    name: jupyterhub
    namespace: default
---
# ConfigMap for JupyterHub configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyterhub-config
  namespace: default
data:
  jupyter_notebook_config.py: |
    c = get_config()
    # ServerApp config (JupyterLab 4.x / Jupyter Server 2.x)
    c.ServerApp.ip = '0.0.0.0'
    c.ServerApp.allow_remote_access = True
    # Disable authentication for dev (enable for production)
    c.IdentityProvider.token = ''

    # Notebook Persistence with MinIO using s3contents
    import os
    from s3contents import S3ContentsManager
    c.ServerApp.contents_manager_class = S3ContentsManager
    c.S3ContentsManager.access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
    c.S3ContentsManager.secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
    c.S3ContentsManager.endpoint_url = os.environ.get('JUPYTER_S3_ENDPOINT')
    c.S3ContentsManager.bucket = "notebooks"
    c.S3ContentsManager.prefix = "jupyterhub"
    # Disable SSL verification for internal MinIO (if needed, s3contents uses session_kwargs)
    c.S3ContentsManager.session_kwargs = {"verify": False}

  setup-kernels.sh: |
    #!/bin/bash
    set -e

    echo "=== Spark Connect Thin Client Setup ==="
    echo "Spark Remote: ${SPARK_REMOTE}"

    # Create PySpark kernel for Jupyter (Spark Connect mode)
    mkdir -p /home/jovyan/.local/share/jupyter/kernels/pyspark_connect
    cat > /home/jovyan/.local/share/jupyter/kernels/pyspark_connect/kernel.json << 'KERNEL_EOF'
    {
      "argv": ["python", "-m", "ipykernel_launcher", "-f", "{connection_file}"],
      "display_name": "PySpark (Connect)",
      "language": "python",
      "env": {
        "PYSPARK_PYTHON": "python3",
        "SPARK_REMOTE": "${SPARK_REMOTE}",
        "MINIO_ENDPOINT": "${MINIO_ENDPOINT}",
        "AWS_ACCESS_KEY_ID": "${AWS_ACCESS_KEY_ID}",
        "AWS_SECRET_ACCESS_KEY": "${AWS_SECRET_ACCESS_KEY}"
      }
    }
    KERNEL_EOF

    # Create IPython startup script for auto-initialization
    mkdir -p /home/jovyan/.ipython/profile_default/startup/
    cat > /home/jovyan/.ipython/profile_default/startup/00-pyspark-setup.py << 'STARTUP_EOF'
    import os

    spark_remote = os.environ.get('SPARK_REMOTE')

    if spark_remote:
        try:
            from pyspark.sql import SparkSession
            
            # Connect to remote Spark Connect server
            spark = SparkSession.builder.remote(spark_remote).getOrCreate()
            
            # Make spark available globally
            import builtins
            builtins.spark = spark
            
            print(f"✅ Connected to Spark {spark.version} via Spark Connect")
            print(f"   Remote: {spark_remote}")
        except Exception as e:
            print(f"⚠️ Spark Connect initialization failed: {e}")
            print(f"   Ensure Spark Connect Server is running at {spark_remote}")
    else:
        print("⚠️ SPARK_REMOTE not set. Spark session not initialized.")
    STARTUP_EOF

    echo "=== Spark Connect configuration complete ==="
