apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupyterhub
  namespace: default
  labels:
    app: jupyterhub
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupyterhub
  template:
    metadata:
      labels:
        app: jupyterhub
        spark-role: driver
    spec:
      serviceAccountName: jupyterhub
      initContainers:
        # Copy Spark binaries and set permissions for jovyan user (UID 1000)
        - name: spark-home-init
          image: subhodeep2022/spark-bigdata:spark-4.0.1-uc-0.3.1-fix-v4
          imagePullPolicy: Always
          command: ["sh", "-c", "cp -r /opt/spark/* /mnt/spark/ && chown -R 1000:100 /mnt/spark"]
          volumeMounts:
            - name: spark-home
              mountPath: /mnt/spark

      containers:
        - name: jupyterhub
          image: subhodeep2022/spark-bigdata:jupyterhub-4.0.7-pyspark-scala-sql-v2
          imagePullPolicy: Always
          ports:
            - containerPort: 8888
              name: web
            - containerPort: 4040
              name: spark-ui
          env:
            # Spark Configuration
            - name: SPARK_HOME
              value: "/opt/spark"
            - name: PYSPARK_PYTHON
              value: "python3"
            - name: PYSPARK_DRIVER_PYTHON
              value: "python3"
            - name: SPARK_IMAGE
              value: "subhodeep2022/spark-bigdata:spark-4.0.1-uc-0.3.1-fix-v4"
            - name: SPARK_REMOTE
              value: "sc://spark-connect-server-driver-svc:15002"
            
            # Spark on K8s Configuration (Not needed for Connect client, but kept for non-remote fallback)
            - name: SPARK_DRIVER_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            
            # S3/MinIO Configuration (Referencing global-config ConfigMap)
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: AWS_SECRET_ACCESS_KEY
            - name: JUPYTER_S3_ENDPOINT
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: MINIO_ENDPOINT
            - name: MINIO_ENDPOINT
              valueFrom:
                configMapKeyRef:
                  name: global-config
                  key: MINIO_ENDPOINT
            
            # Jupyter Configuration
            - name: JUPYTER_ENABLE_LAB
              value: "yes"
            - name: JUPYTER_TOKEN
              value: ""  # No token for dev (set for production)
            - name: GRANT_SUDO
              value: "yes"
            

          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "2000m"
          
          volumeMounts:
            # Spark binaries
            - name: spark-home
              mountPath: /opt/spark
            # Spark config template (copied and extended at startup)
            - name: spark-config
              mountPath: /tmp/spark-config-template/spark-defaults.conf
              subPath: spark-defaults.conf
            # JupyterHub config (mounted to standard Docker Stacks location)
            - name: jupyter-config
              mountPath: /etc/jupyter/jupyter_notebook_config.py
              subPath: jupyter_notebook_config.py
            # Notebooks persistence (S3 sync handled by script)
            - name: notebooks
              mountPath: /home/jovyan/work
            # Startup script for kernel setup
            - name: startup-script
              mountPath: /usr/local/bin/before-notebook.d/setup-kernels.sh
              subPath: setup-kernels.sh

      volumes:
        - name: spark-home
          emptyDir: {}
        - name: spark-config
          configMap:
            name: spark-config
        - name: jupyter-config
          configMap:
            name: jupyterhub-config
        - name: notebooks
          emptyDir: {}  # Use PVC or S3 sync for persistence
        - name: startup-script
          configMap:
            name: jupyterhub-config
            defaultMode: 0755
---
# Service for JupyterHub Web UI
apiVersion: v1
kind: Service
metadata:
  name: jupyterhub
  namespace: default
spec:
  selector:
    app: jupyterhub
  ports:
    - protocol: TCP
      port: 8888
      targetPort: 8888
      name: web
    - protocol: TCP
      port: 4040
      targetPort: 4040
      name: spark-ui
  type: ClusterIP
---
# Headless Service for Spark Driver communication
apiVersion: v1
kind: Service
metadata:
  name: jupyterhub-driver
  namespace: default
spec:
  clusterIP: None
  selector:
    app: jupyterhub
  ports:
    - port: 22321
      name: spark-driver
    - port: 22322
      name: spark-blockmanager
    - port: 4040
      name: spark-ui
---
# ServiceAccount for JupyterHub
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jupyterhub
  namespace: default
---
# ClusterRoleBinding for Spark executor management
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jupyterhub-spark
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin  # For dev; use restricted role in production
subjects:
  - kind: ServiceAccount
    name: jupyterhub
    namespace: default
---
# ConfigMap for JupyterHub configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyterhub-config
  namespace: default
data:
  jupyter_notebook_config.py: |
    c = get_config()
    # ServerApp config (JupyterLab 4.x / Jupyter Server 2.x)
    c.ServerApp.ip = '0.0.0.0'
    c.ServerApp.allow_remote_access = True
    # Disable authentication for dev (enable for production)
    c.IdentityProvider.token = ''

    # Notebook Persistence with MinIO using s3contents
    import os
    from s3contents import S3ContentsManager
    c.ServerApp.contents_manager_class = S3ContentsManager
    c.S3ContentsManager.access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
    c.S3ContentsManager.secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
    c.S3ContentsManager.endpoint_url = os.environ.get('JUPYTER_S3_ENDPOINT')
    c.S3ContentsManager.bucket = "notebooks"
    c.S3ContentsManager.prefix = "jupyterhub"
    # Disable SSL verification for internal MinIO (if needed, s3contents uses session_kwargs)
    c.S3ContentsManager.session_kwargs = {"verify": False}
    
  setup-kernels.sh: |
    #!/bin/bash
    set -e
    
    echo "=== Spark-on-Kubernetes Dynamic Configuration ==="
    
    # CRITICAL: Generate dynamic spark-defaults.conf with driver host
    # The ConfigMap spark-defaults.conf is read-only, so we copy and extend it
    # The pod IP (SPARK_DRIVER_HOST) is only known at runtime
    
    SPARK_CONF_DIR="/opt/spark/conf"
    TEMPLATE_CONF="/tmp/spark-config-template/spark-defaults.conf"
    FINAL_CONF="${SPARK_CONF_DIR}/spark-defaults.conf"
    
    echo "Pod IP: ${SPARK_DRIVER_HOST}"
    
    # Create conf directory if not exists
    mkdir -p "${SPARK_CONF_DIR}"
    
    # Copy base config from template (mounted read-only from ConfigMap)
    if [ -f "${TEMPLATE_CONF}" ]; then
      cp "${TEMPLATE_CONF}" "${FINAL_CONF}"
      echo "Copied base config from template"
    else
      echo "WARNING: No template config found, creating minimal config"
      touch "${FINAL_CONF}"
    fi
    
    # Append dynamic driver configuration (CRITICAL for executor connectivity)
    cat >> "${FINAL_CONF}" << 'DRIVER_EOF'

    # === DYNAMIC DRIVER CONFIG (generated at startup) ===
    # These settings are critical for executor-to-driver communication
    DRIVER_EOF

    # Use echo to append with variable expansion
    echo "spark.driver.host                   ${SPARK_DRIVER_HOST}" >> "${FINAL_CONF}"
    echo "spark.driver.bindAddress            0.0.0.0" >> "${FINAL_CONF}"
    echo "spark.driver.port                   22321" >> "${FINAL_CONF}"
    echo "spark.blockManager.port             22322" >> "${FINAL_CONF}"
    # CRITICAL: Client mode requires driver pod name to match actual pod name
    echo "spark.kubernetes.driver.pod.name    ${HOSTNAME}" >> "${FINAL_CONF}"
    echo "spark.kubernetes.container.image    ${SPARK_IMAGE}" >> "${FINAL_CONF}"
    echo "spark.hadoop.fs.s3a.endpoint        ${MINIO_ENDPOINT}" >> "${FINAL_CONF}"
    echo "spark.hadoop.fs.s3a.access.key      ${AWS_ACCESS_KEY_ID}" >> "${FINAL_CONF}"
    echo "spark.hadoop.fs.s3a.secret.key      ${AWS_SECRET_ACCESS_KEY}" >> "${FINAL_CONF}"
    
    # Python executable path for driver and executors
    echo "spark.pyspark.python                python3" >> "${FINAL_CONF}"
    echo "spark.pyspark.driver.python          python3" >> "${FINAL_CONF}"
    echo "spark.executorEnv.PYSPARK_PYTHON     python3" >> "${FINAL_CONF}"
    
    chmod 644 "${FINAL_CONF}"
    
    echo "Generated spark-defaults.conf with driver.host=${SPARK_DRIVER_HOST}"
    echo "Driver config:"
    cat "${FINAL_CONF}" | grep -E "^spark\.(driver|blockManager)" || true
    
    # Create PySpark kernel for Jupyter (specialized for K8s)
    mkdir -p /home/jovyan/.local/share/jupyter/kernels/pyspark_k8s
    cat > /home/jovyan/.local/share/jupyter/kernels/pyspark_k8s/kernel.json << 'KERNEL_EOF'
    {
      "argv": ["python", "-m", "ipykernel_launcher", "-f", "{connection_file}"],
      "display_name": "PySpark (Connect)",
      "language": "python",
      "env": {
        "SPARK_HOME": "/opt/spark",
        "PYSPARK_PYTHON": "python3",
        "PYSPARK_DRIVER_PYTHON": "python3",
        "SPARK_REMOTE": "sc://spark-connect-server-driver-svc:15002",
        "MINIO_ENDPOINT": "${MINIO_ENDPOINT}",
        "AWS_ACCESS_KEY_ID": "${AWS_ACCESS_KEY_ID}",
        "AWS_SECRET_ACCESS_KEY": "${AWS_SECRET_ACCESS_KEY}"
      }
    }
    KERNEL_EOF

    # Create IPython startup script (Best for Jupyter Notebooks)
    mkdir -p /home/jovyan/.ipython/profile_default/startup/
    cat > /home/jovyan/.ipython/profile_default/startup/00-pyspark-setup.py << 'STARTUP_EOF'
    import os
    import sys
    import glob

    spark_remote = os.environ.get('SPARK_REMOTE')
    spark_home = os.environ.get('SPARK_HOME', '/opt/spark')
    
    # Add Spark Python paths
    sys.path.insert(0, os.path.join(spark_home, 'python'))
    py4j_zip = glob.glob(os.path.join(spark_home, 'python/lib/py4j-*-src.zip'))
    if py4j_zip:
        sys.path.insert(0, py4j_zip[0])

    try:
        from pyspark.sql import SparkSession
        builder = SparkSession.builder
        if spark_remote:
            builder = builder.remote(spark_remote)
        else:
            builder = builder.enableHiveSupport()
        
        spark = builder.getOrCreate()
        
        import builtins
        builtins.spark = spark
        print(f"✅ Auto-initialized Spark {spark.version} (Remote: {spark_remote})")
    except Exception as e:
        print(f"⚠️ Spark auto-init failed: {e}")
    STARTUP_EOF
    
    # Create sitecustomize.py for path injection (robust)
    SITE_PACKAGES="/home/jovyan/.local/lib/python3.11/site-packages"
    mkdir -p "${SITE_PACKAGES}"
    
    cat > "${SITE_PACKAGES}/sitecustomize.py" << 'SITE_EOF'
    import os
    import sys
    import glob
    
    # Ensure Spark is in path for all Python processes
    spark_home = os.environ.get('SPARK_HOME', '/opt/spark')
    if spark_home:
        sys.path.insert(0, os.path.join(spark_home, 'python'))
        # Dynamically find py4j zip
        py4j_zip = glob.glob(os.path.join(spark_home, 'python/lib/py4j-*-src.zip'))
        if py4j_zip:
            sys.path.insert(0, py4j_zip[0])
    SITE_EOF


    echo "=== Spark configuration complete ==="
