# Default Spark Configuration for Kubernetes
# This file is mounted into the Zeppelin and Spark pods via ConfigMap.

# Master Address (K8s: spark-master service)
# Master Address (Spark on Kubernetes)
spark.master                     k8s://https://kubernetes.default.svc

# Resource Allocation (Dynamic Scaling)
# Starts with 1 executor and scales up to 4 based on workload.
spark.dynamicAllocation.enabled          true
spark.dynamicAllocation.shuffleTracking.enabled true
spark.dynamicAllocation.minExecutors     1
spark.dynamicAllocation.maxExecutors     4
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.schedulerBacklogTimeout 1s

# Per-Executor Resources
spark.executor.cores             4
spark.executor.memory            4g
spark.driver.cores               1
spark.driver.memory              2g

# Event Logging
# Disabled to avoid errors with missing directories on ephemeral K8s storage.
# For production usage, point this to S3 or a PVC-mounted directory.
spark.eventLog.enabled           false
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

# Prometheus Metrics (Enabled for observability)
spark.ui.prometheus.enabled      true
spark.metrics.namespace          spark
spark.servicemonitor.port        4040
spark.metrics.conf.*.sink.prometheusServlet.class org.apache.spark.metrics.sink.PrometheusServlet
spark.metrics.conf.*.sink.prometheusServlet.path /metrics/prometheus

# Annotations for Prometheus Scraping (Driver & Executor)
spark.kubernetes.driver.annotation.prometheus.io/scrape true
spark.kubernetes.driver.annotation.prometheus.io/path   /metrics/prometheus
spark.kubernetes.driver.annotation.prometheus.io/port   4040
spark.kubernetes.executor.annotation.prometheus.io/scrape true
spark.kubernetes.executor.annotation.prometheus.io/path   /metrics/prometheus
spark.kubernetes.executor.annotation.prometheus.io/port   4040

# Kubernetes Configuration
spark.kubernetes.container.image subhodeep2022/spark-bigdata:spark-3.5.7-rocksdb
spark.kubernetes.namespace       default
spark.kubernetes.authenticate.driver.serviceAccountName zeppelin-server

# S3 & Hive Configuration
# (Currently commented out for initial development/stability)
# Uncomment these sections when MinIO and Hive Metastore are ready.
# S3 & Hive Configuration
# Enabled for Full Stack Architecture
spark.hadoop.fs.s3a.impl                 org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint             http://minio.default.svc.cluster.local:9000
spark.hadoop.fs.s3a.access.key           minioadmin
spark.hadoop.fs.s3a.secret.key           minioadmin
spark.hadoop.fs.s3a.path.style.access    true
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# Delta Lake Configuration
# spark.sql.extensions                     io.delta.sql.DeltaSparkSessionExtension
# spark.sql.catalog.spark_catalog          org.apache.spark.sql.delta.catalog.DeltaCatalog

# Hive Metastore Configuration
# Uses direct Thrift connection to Hive Metastore Service
spark.sql.catalogImplementation          hive
spark.hadoop.hive.metastore.uris         thrift://hive-metastore.default.svc.cluster.local:9083

# Driver & Executor Config
spark.driver.extraClassPath              /opt/spark/custom-jars/postgresql-42.6.0.jar
spark.executor.extraClassPath            /opt/spark/custom-jars/postgresql-42.6.0.jar
spark.driver.extraJavaOptions            -Djava.net.preferIPv4Stack=true -Djava.security.egd=file:/dev/./urandom -XX:+TieredCompilation -XX:TieredStopAtLevel=1
spark.executor.extraJavaOptions          -Djava.net.preferIPv4Stack=true -Djava.security.egd=file:/dev/./urandom -XX:+TieredCompilation -XX:TieredStopAtLevel=1

# Performance Tuning
spark.cores.max                          4
spark.sql.shuffle.partitions             200
spark.sql.adaptive.enabled               true
spark.ui.bindHost                        0.0.0.0
spark.ui.port                            4040
spark.memory.fraction                    0.8
spark.sql.inMemoryColumnarStorage.compressed true

# RocksDB State Store Configuration (Structured Streaming)
spark.sql.streaming.stateStore.providerClass   org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider
spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled true

