# =============================================================================
# Spark Configuration (Kubernetes)
# =============================================================================

# --- Application & Naming ---
spark.app.name                   spark-bigdata
spark.kubernetes.executor.podNamePrefix spark-executor

# --- Master & Deployment ---
spark.master                     k8s://https://kubernetes.default.svc
spark.submit.deployMode          client
spark.kubernetes.namespace       default
spark.kubernetes.authenticate.driver.serviceAccountName jupyterhub

# --- SQL & ANSI Mode ---
# Disable ANSI mode to fix NumberFormatException or similar parsing issues
spark.sql.ansi.enabled           false

# --- Dynamic Resource Allocation ---
spark.dynamicAllocation.enabled              true
spark.dynamicAllocation.shuffleTracking.enabled true
spark.dynamicAllocation.minExecutors         1
spark.dynamicAllocation.maxExecutors         10
# Use bare integers (seconds) to avoid parsing issues
spark.dynamicAllocation.executorIdleTimeout  600
spark.dynamicAllocation.schedulerBacklogTimeout 5

# --- Networking & Timeouts ---
# Set explicit timeouts as integers (seconds) to bypass invalid defaults like "60s"
spark.network.timeout            120
spark.files.fetchTimeout         60
spark.executor.heartbeatInterval 10
spark.rpc.askTimeout             120

# --- Hadoop S3A Configuration (CRITICAL Fixes for Spark 4.0) ---
# Override properties that default to "60s" or "24h" causing NumberFormatException
spark.hadoop.fs.s3a.threads.keepalivetime    60
spark.hadoop.fs.s3a.connection.timeout       200
spark.hadoop.fs.s3a.connection.establish.timeout 30
spark.hadoop.fs.s3a.connection.acquisition.timeout 60
spark.hadoop.fs.s3a.connection.idle.time     60
spark.hadoop.fs.s3a.connection.request.timeout 60
spark.hadoop.fs.s3a.connection.ttl           300
spark.hadoop.fs.s3a.multipart.purge.age      86400
spark.hadoop.fs.s3a.paging.maximum           5000

# --- Executor & Driver Resources ---
spark.executor.cores             1
spark.executor.memory            2g
spark.driver.cores               1
spark.driver.memory              2g

# --- Metrics & Monitoring ---
spark.ui.prometheus.enabled      true
spark.metrics.namespace          spark

# --- SQL & Catalogs (Delta Lake & Hive) ---
# Iceberg extensions removed to fix DELTA_MISSING_ICEBERG_CLASS
spark.sql.extensions             io.delta.sql.DeltaSparkSessionExtension
spark.sql.defaultCatalog         spark_catalog
spark.sql.catalog.spark_catalog  org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.sql.catalogImplementation  hive
spark.hadoop.hive.metastore.uris thrift://hive-metastore.default.svc.cluster.local:9083
spark.sql.warehouse.dir          s3a://warehouse/managed/

# --- Delta Lake Features ---
# UniForm (Iceberg compatibility) disabled completely
# Ensure UniForm is OFF by default
spark.databricks.delta.universalFormat.enabledFormats none
spark.databricks.delta.allowArbitraryProperties.enabled true

# --- S3 / MinIO Configuration ---
spark.hadoop.fs.s3a.impl                 org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3.impl                  org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access    true
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.fast.upload          true

# --- Networking & UI ---
spark.ui.bindHost                        0.0.0.0
spark.ui.port                            4040
spark.driver.bindAddress                 0.0.0.0

# --- Performance Tuning ---
spark.sql.shuffle.partitions             200
spark.sql.adaptive.enabled               true
