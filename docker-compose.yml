version: '3.8'

services:
  spark-master:
    build: ./spark
    container_name: spark-master
    hostname: spark-master
    ports:
      - "${SPARK_MASTER_WEBUI_PORT}:8080"
      - "${SPARK_MASTER_PORT}:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./data/spark-events:/opt/spark/spark-events
      - spark-home:/opt/spark
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    networks:
      - databricks-net

  spark-worker:
    build: ./spark
    container_name: spark-worker
    hostname: spark-worker
    depends_on:
      - spark-master
    ports:
      - "${SPARK_WORKER_WEBUI_PORT}:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:${SPARK_MASTER_PORT}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:${SPARK_MASTER_PORT}
    networks:
      - databricks-net

  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    ports:
      - "${MINIO_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - ./data/minio:/data
    networks:
      - databricks-net

  minio-create-buckets:
    image: minio/mc
    container_name: minio-create-buckets
    depends_on:
      - minio
    volumes:
      - ./minio/create-buckets.sh:/create-buckets.sh
    entrypoint: /bin/sh
    command: -c "chmod +x /create-buckets.sh && /create-buckets.sh"
    networks:
      - databricks-net

  hive-metastore-postgresql:
    image: ${POSTGRES_IMAGE}
    container_name: hive-metastore-postgresql
    hostname: hive-metastore-postgresql
    environment:
      - POSTGRES_DB=${HIVE_METASTORE_DB_NAME}
      - POSTGRES_USER=${HIVE_METASTORE_DB_USER}
      - POSTGRES_PASSWORD=${HIVE_METASTORE_DB_PASSWORD}
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    networks:
      - databricks-net

  hive-metastore:
    build: ./hive
    container_name: hive-metastore
    hostname: hive-metastore
    depends_on:
      - hive-metastore-postgresql
      - minio
    ports:
      - "${HIVE_METASTORE_PORT}:9083"
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - SKIP_SCHEMA_INIT=true
    entrypoint:
      - "/bin/bash"
      - "-c"
    volumes:
      - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
    command:
      - "/opt/hive/bin/schematool -dbType postgres -validate || /opt/hive/bin/schematool -dbType postgres -initSchema; /opt/hive/bin/hive --service metastore"
    networks:
      - databricks-net

  hive-server:
    build: ./spark
    container_name: hive-server
    hostname: hive-server
    restart: unless-stopped
    depends_on:
      - hive-metastore
      - spark-master
    ports:
      - "${HIVE_SERVER_THRIFT_PORT}:10000"
      - "${HIVE_SERVER_WEBUI_PORT}:10002"
    environment:
      - SPARK_MASTER=spark://spark-master:${SPARK_MASTER_PORT}
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
    volumes:
      - ./hive/conf/hive-site-hs2.xml:/opt/spark/conf/hive-site.xml
      - ./scripts/start-hive-server.sh:/start-hive-server.sh
    command: /bin/bash /start-hive-server.sh
    networks:
      databricks-net:

  airflow-webserver:
    image: ${AIRFLOW_IMAGE}
    container_name: airflow-webserver
    hostname: airflow-webserver
    depends_on:
      - airflow-scheduler
    ports:
      - "${AIRFLOW_WEB_PORT}:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW_EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=True
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_ADMIN_USER}
      - _AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_ADMIN_PASSWORD}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: webserver
    networks:
      - databricks-net

  airflow-scheduler:
    image: ${AIRFLOW_IMAGE}
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    depends_on:
      - airflow-postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW_EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=True
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: scheduler
    networks:
      - databricks-net

  airflow-postgres:
    image: ${POSTGRES_IMAGE}
    container_name: airflow-postgres
    hostname: airflow-postgres
    environment:
      - POSTGRES_USER=${AIRFLOW_DB_USER}
      - POSTGRES_PASSWORD=${AIRFLOW_DB_PASSWORD}
      - POSTGRES_DB=${AIRFLOW_DB_NAME}
    volumes:
      - ./data/airflow-postgres:/var/lib/postgresql/data
    networks:
      - databricks-net

  zeppelin:
    image: ${ZEPPELIN_IMAGE}
    container_name: zeppelin
    hostname: zeppelin
    user: root
    depends_on:
      - spark-master
    ports:
      - "${ZEPPELIN_PORT}:8080"
      - "4040:4040"  # Spark UI
    environment:
      - ZEPPELIN_ADDR=${ZEPPELIN_ADDR}
      - SPARK_MASTER=spark://spark-master:${SPARK_MASTER_PORT}
      - SPARK_HOME=/opt/spark
      - ZEPPELIN_SPARK_ENABLESUPPORTEDVERSIONCHECK=false
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
    volumes:
      - ./data/zeppelin/notebook:/opt/zeppelin/notebook
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./data/spark-events:/opt/spark/spark-events
      - ./spark/jars:/opt/spark/custom-jars
      - spark-home:/opt/spark
    networks:
      - databricks-net

  code-server:
    image: ${CODE_SERVER_IMAGE}
    container_name: code-server
    ports:
      - "${CODE_SERVER_PORT}:8080"
    environment:
      - PASSWORD=${CODE_SERVER_PASSWORD}
    volumes:
      - ./:/home/coder/project
      - ./airflow/dags:/home/coder/dags
    networks:
      - databricks-net

networks:
  databricks-net:
    name: databricks-net
    driver: bridge

volumes:
  spark-home:
