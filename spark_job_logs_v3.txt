>>> DIAGNOSTIC: Checking for AWS SDK Jars...
-rw-r--r-- 1 root root 368914230 Jan 16  2024 aws-java-sdk-bundle-1.12.638.jar
-rw-r--r-- 1 root root 590059756 Oct 24  2024 bundle-2.29.0.jar
-rw-r--r-- 1 root root    822359 Mar  4  2024 hadoop-aws-3.4.0.jar
>>> DIAGNOSTIC: Done.
WARNING: Using incubator modules: jdk.incubator.vector
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2.5.2/cache
The jars for the packages stored in: /root/.ivy2.5.2/jars
io.unitycatalog#unitycatalog-spark_2.13 added as a dependency
io.delta#delta-spark_2.13 added as a dependency
org.apache.iceberg#iceberg-spark-runtime-3.5_2.13 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-08769bf1-a3a3-44d6-91a5-8ab2468ba03b;1.0
	confs: [default]
	found io.unitycatalog#unitycatalog-spark_2.13;0.3.1 in central
	found io.unitycatalog#unitycatalog-client;0.3.1 in central
	found org.slf4j#slf4j-api;2.0.13 in central
	found org.apache.logging.log4j#log4j-slf4j2-impl;2.24.3 in central
	found org.apache.logging.log4j#log4j-api;2.24.3 in central
	found org.apache.logging.log4j#log4j-core;2.24.3 in central
	found com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 in central
	found org.openapitools#jackson-databind-nullable;0.2.6 in central
	found com.google.code.findbugs#jsr305;3.0.2 in central
	found com.fasterxml.jackson.core#jackson-databind;2.15.0 in central
	found com.fasterxml.jackson.core#jackson-annotations;2.15.0 in central
	found com.fasterxml.jackson.core#jackson-core;2.15.0 in central
	found com.fasterxml.jackson.module#jackson-module-scala_2.13;2.15.0 in central
	found com.thoughtworks.paranamer#paranamer;2.8 in central
	found com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 in central
	found org.codehaus.woodstox#stax2-api;4.2.1 in central
	found com.fasterxml.woodstox#woodstox-core;6.5.1 in central
	found org.antlr#antlr4-runtime;4.13.1 in central
	found org.antlr#antlr4;4.13.1 in central
	found org.antlr#antlr-runtime;3.5.3 in central
	found org.antlr#ST4;4.3.4 in central
	found org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central
	found com.ibm.icu#icu4j;72.1 in central
	found org.apache.hadoop#hadoop-client-runtime;3.4.0 in central
	found org.apache.hadoop#hadoop-client-api;3.4.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.4 in central
	found commons-logging#commons-logging;1.2 in central
	found io.delta#delta-spark_2.13;4.0.0 in central
	found io.delta#delta-storage;4.0.0 in central
	found org.apache.iceberg#iceberg-spark-runtime-3.5_2.13;1.7.0 in central
downloading https://repo1.maven.org/maven2/io/unitycatalog/unitycatalog-spark_2.13/0.3.1/unitycatalog-spark_2.13-0.3.1.jar ...
	[SUCCESSFUL ] io.unitycatalog#unitycatalog-spark_2.13;0.3.1!unitycatalog-spark_2.13.jar (89ms)
downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar ...
	[SUCCESSFUL ] io.delta#delta-spark_2.13;4.0.0!delta-spark_2.13.jar (264ms)
downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.13/1.7.0/iceberg-spark-runtime-3.5_2.13-1.7.0.jar ...
	[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.13;1.7.0!iceberg-spark-runtime-3.5_2.13.jar (375ms)
downloading https://repo1.maven.org/maven2/io/unitycatalog/unitycatalog-client/0.3.1/unitycatalog-client-0.3.1.jar ...
	[SUCCESSFUL ] io.unitycatalog#unitycatalog-client;0.3.1!unitycatalog-client.jar (82ms)
downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.13/slf4j-api-2.0.13.jar ...
	[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.13!slf4j-api.jar (89ms)
downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-slf4j2-impl/2.24.3/log4j-slf4j2-impl-2.24.3.jar ...
	[SUCCESSFUL ] org.apache.logging.log4j#log4j-slf4j2-impl;2.24.3!log4j-slf4j2-impl.jar (89ms)
downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.24.3/log4j-api-2.24.3.jar ...
	[SUCCESSFUL ] org.apache.logging.log4j#log4j-api;2.24.3!log4j-api.jar (85ms)
downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.15.0/jackson-databind-2.15.0.jar ...
	[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.15.0!jackson-databind.jar (89ms)
downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.13/2.15.0/jackson-module-scala_2.13-2.15.0.jar ...
	[SUCCESSFUL ] com.fasterxml.jackson.module#jackson-module-scala_2.13;2.15.0!jackson-module-scala_2.13.jar(bundle) (88ms)
downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.15.0/jackson-annotations-2.15.0.jar ...
	[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.15.0!jackson-annotations.jar (93ms)
downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.15.0/jackson-core-2.15.0.jar ...
	[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.15.0!jackson-core.jar (72ms)
downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.15.0/jackson-dataformat-xml-2.15.0.jar ...
	[SUCCESSFUL ] com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0!jackson-dataformat-xml.jar (78ms)
downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.13.1/antlr4-runtime-4.13.1.jar ...
	[SUCCESSFUL ] org.antlr#antlr4-runtime;4.13.1!antlr4-runtime.jar (73ms)
downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.13.1/antlr4-4.13.1.jar ...
	[SUCCESSFUL ] org.antlr#antlr4;4.13.1!antlr4.jar (93ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.4.0/hadoop-client-runtime-3.4.0.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.4.0!hadoop-client-runtime.jar (227ms)
downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.17.0/jackson-datatype-jsr310-2.17.0.jar ...
	[SUCCESSFUL ] com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0!jackson-datatype-jsr310.jar(bundle) (73ms)
downloading https://repo1.maven.org/maven2/org/openapitools/jackson-databind-nullable/0.2.6/jackson-databind-nullable-0.2.6.jar ...
	[SUCCESSFUL ] org.openapitools#jackson-databind-nullable;0.2.6!jackson-databind-nullable.jar (68ms)
downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...
	[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (77ms)
downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.24.3/log4j-core-2.24.3.jar ...
	[SUCCESSFUL ] org.apache.logging.log4j#log4j-core;2.24.3!log4j-core.jar (81ms)
downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...
	[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (77ms)
downloading https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar ...
	[SUCCESSFUL ] org.codehaus.woodstox#stax2-api;4.2.1!stax2-api.jar(bundle) (79ms)
downloading https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/6.5.1/woodstox-core-6.5.1.jar ...
	[SUCCESSFUL ] com.fasterxml.woodstox#woodstox-core;6.5.1!woodstox-core.jar(bundle) (85ms)
downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.3/antlr-runtime-3.5.3.jar ...
	[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.3!antlr-runtime.jar (76ms)
downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.3.4/ST4-4.3.4.jar ...
	[SUCCESSFUL ] org.antlr#ST4;4.3.4!ST4.jar (74ms)
downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...
	[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (79ms)
downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/72.1/icu4j-72.1.jar ...
	[SUCCESSFUL ] com.ibm.icu#icu4j;72.1!icu4j.jar (122ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.4.0/hadoop-client-api-3.4.0.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.4.0!hadoop-client-api.jar (150ms)
downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.4/snappy-java-1.1.10.4.jar ...
	[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.4!snappy-java.jar(bundle) (93ms)
downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar ...
	[SUCCESSFUL ] commons-logging#commons-logging;1.2!commons-logging.jar (237ms)
downloading https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar ...
	[SUCCESSFUL ] io.delta#delta-storage;4.0.0!delta-storage.jar (110ms)
:: resolution report :: resolve 15911ms :: artifacts dl 3407ms
	:: modules in use:
	com.fasterxml.jackson.core#jackson-annotations;2.15.0 from central in [default]
	com.fasterxml.jackson.core#jackson-core;2.15.0 from central in [default]
	com.fasterxml.jackson.core#jackson-databind;2.15.0 from central in [default]
	com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.15.0 from central in [default]
	com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.17.0 from central in [default]
	com.fasterxml.jackson.module#jackson-module-scala_2.13;2.15.0 from central in [default]
	com.fasterxml.woodstox#woodstox-core;6.5.1 from central in [default]
	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
	com.ibm.icu#icu4j;72.1 from central in [default]
	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
	commons-logging#commons-logging;1.2 from central in [default]
	io.delta#delta-spark_2.13;4.0.0 from central in [default]
	io.delta#delta-storage;4.0.0 from central in [default]
	io.unitycatalog#unitycatalog-client;0.3.1 from central in [default]
	io.unitycatalog#unitycatalog-spark_2.13;0.3.1 from central in [default]
	org.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]
	org.antlr#ST4;4.3.4 from central in [default]
	org.antlr#antlr-runtime;3.5.3 from central in [default]
	org.antlr#antlr4;4.13.1 from central in [default]
	org.antlr#antlr4-runtime;4.13.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.4.0 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.4.0 from central in [default]
	org.apache.iceberg#iceberg-spark-runtime-3.5_2.13;1.7.0 from central in [default]
	org.apache.logging.log4j#log4j-api;2.24.3 from central in [default]
	org.apache.logging.log4j#log4j-core;2.24.3 from central in [default]
	org.apache.logging.log4j#log4j-slf4j2-impl;2.24.3 from central in [default]
	org.codehaus.woodstox#stax2-api;4.2.1 from central in [default]
	org.openapitools#jackson-databind-nullable;0.2.6 from central in [default]
	org.slf4j#slf4j-api;2.0.13 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.4 from central in [default]
	:: evicted modules:
	org.slf4j#slf4j-api;2.0.16 by [org.slf4j#slf4j-api;2.0.13] in [default]
	com.fasterxml.jackson.core#jackson-annotations;2.17.0 by [com.fasterxml.jackson.core#jackson-annotations;2.15.0] in [default]
	com.fasterxml.jackson.core#jackson-core;2.17.0 by [com.fasterxml.jackson.core#jackson-core;2.15.0] in [default]
	com.fasterxml.jackson.core#jackson-databind;2.17.0 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]
	com.fasterxml.jackson.core#jackson-databind;2.14.0-rc2 by [com.fasterxml.jackson.core#jackson-databind;2.15.0] in [default]
	org.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.13] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   36  |   30  |   30  |   6   ||   30  |   30  |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-08769bf1-a3a3-44d6-91a5-8ab2468ba03b
	confs: [default]
	30 artifacts copied, 0 already retrieved (124568kB/156ms)
26/01/12 18:59:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting Lakehouse Verification...
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
26/01/12 18:59:41 INFO SparkContext: Running Spark version 4.0.1
26/01/12 18:59:41 INFO SparkContext: OS info Linux, 6.6.113+, amd64
26/01/12 18:59:41 INFO SparkContext: Java version 17.0.17
26/01/12 18:59:41 INFO ResourceUtils: ==============================================================
26/01/12 18:59:41 INFO ResourceUtils: No custom resources configured for spark.driver.
26/01/12 18:59:41 INFO ResourceUtils: ==============================================================
26/01/12 18:59:41 INFO SparkContext: Submitted application: LakehouseVerification
26/01/12 18:59:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
26/01/12 18:59:41 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
26/01/12 18:59:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
26/01/12 18:59:41 INFO SecurityManager: Changing view acls to: root
26/01/12 18:59:41 INFO SecurityManager: Changing modify acls to: root
26/01/12 18:59:41 INFO SecurityManager: Changing view acls groups to: root
26/01/12 18:59:41 INFO SecurityManager: Changing modify acls groups to: root
26/01/12 18:59:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
26/01/12 18:59:41 INFO Utils: Successfully started service 'sparkDriver' on port 39571.
26/01/12 18:59:41 INFO SparkEnv: Registering MapOutputTracker
26/01/12 18:59:41 INFO SparkEnv: Registering BlockManagerMaster
26/01/12 18:59:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
26/01/12 18:59:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
26/01/12 18:59:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
26/01/12 18:59:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d13880c-0ca6-4749-b2e5-1f485709c9b6
26/01/12 18:59:42 INFO SparkEnv: Registering OutputCommitCoordinator
26/01/12 18:59:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
26/01/12 18:59:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/io.unitycatalog_unitycatalog-spark_2.13-0.3.1.jar at spark://10.100.1.243:39571/jars/io.unitycatalog_unitycatalog-spark_2.13-0.3.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/io.delta_delta-spark_2.13-4.0.0.jar at spark://10.100.1.243:39571/jars/io.delta_delta-spark_2.13-4.0.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.13-1.7.0.jar at spark://10.100.1.243:39571/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.13-1.7.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/io.unitycatalog_unitycatalog-client-0.3.1.jar at spark://10.100.1.243:39571/jars/io.unitycatalog_unitycatalog-client-0.3.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.slf4j_slf4j-api-2.0.13.jar at spark://10.100.1.243:39571/jars/org.slf4j_slf4j-api-2.0.13.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.24.3.jar at spark://10.100.1.243:39571/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.24.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-api-2.24.3.jar at spark://10.100.1.243:39571/jars/org.apache.logging.log4j_log4j-api-2.24.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar at spark://10.100.1.243:39571/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.module_jackson-module-scala_2.13-2.15.0.jar at spark://10.100.1.243:39571/jars/com.fasterxml.jackson.module_jackson-module-scala_2.13-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar at spark://10.100.1.243:39571/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar at spark://10.100.1.243:39571/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar at spark://10.100.1.243:39571/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.antlr_antlr4-runtime-4.13.1.jar at spark://10.100.1.243:39571/jars/org.antlr_antlr4-runtime-4.13.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.antlr_antlr4-4.13.1.jar at spark://10.100.1.243:39571/jars/org.antlr_antlr4-4.13.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar at spark://10.100.1.243:39571/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar at spark://10.100.1.243:39571/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar at spark://10.100.1.243:39571/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://10.100.1.243:39571/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-core-2.24.3.jar at spark://10.100.1.243:39571/jars/org.apache.logging.log4j_log4j-core-2.24.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://10.100.1.243:39571/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar at spark://10.100.1.243:39571/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar at spark://10.100.1.243:39571/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.antlr_antlr-runtime-3.5.3.jar at spark://10.100.1.243:39571/jars/org.antlr_antlr-runtime-3.5.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.antlr_ST4-4.3.4.jar at spark://10.100.1.243:39571/jars/org.antlr_ST4-4.3.4.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar at spark://10.100.1.243:39571/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/com.ibm.icu_icu4j-72.1.jar at spark://10.100.1.243:39571/jars/com.ibm.icu_icu4j-72.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar at spark://10.100.1.243:39571/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar at spark://10.100.1.243:39571/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/commons-logging_commons-logging-1.2.jar at spark://10.100.1.243:39571/jars/commons-logging_commons-logging-1.2.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added JAR file:///root/.ivy2.5.2/jars/io.delta_delta-storage-4.0.0.jar at spark://10.100.1.243:39571/jars/io.delta_delta-storage-4.0.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/io.unitycatalog_unitycatalog-spark_2.13-0.3.1.jar at spark://10.100.1.243:39571/files/io.unitycatalog_unitycatalog-spark_2.13-0.3.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/io.unitycatalog_unitycatalog-spark_2.13-0.3.1.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/io.unitycatalog_unitycatalog-spark_2.13-0.3.1.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/io.delta_delta-spark_2.13-4.0.0.jar at spark://10.100.1.243:39571/files/io.delta_delta-spark_2.13-4.0.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/io.delta_delta-spark_2.13-4.0.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/io.delta_delta-spark_2.13-4.0.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.13-1.7.0.jar at spark://10.100.1.243:39571/files/org.apache.iceberg_iceberg-spark-runtime-3.5_2.13-1.7.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.13-1.7.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.apache.iceberg_iceberg-spark-runtime-3.5_2.13-1.7.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/io.unitycatalog_unitycatalog-client-0.3.1.jar at spark://10.100.1.243:39571/files/io.unitycatalog_unitycatalog-client-0.3.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/io.unitycatalog_unitycatalog-client-0.3.1.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/io.unitycatalog_unitycatalog-client-0.3.1.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.slf4j_slf4j-api-2.0.13.jar at spark://10.100.1.243:39571/files/org.slf4j_slf4j-api-2.0.13.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.slf4j_slf4j-api-2.0.13.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.slf4j_slf4j-api-2.0.13.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.24.3.jar at spark://10.100.1.243:39571/files/org.apache.logging.log4j_log4j-slf4j2-impl-2.24.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-slf4j2-impl-2.24.3.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.apache.logging.log4j_log4j-slf4j2-impl-2.24.3.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-api-2.24.3.jar at spark://10.100.1.243:39571/files/org.apache.logging.log4j_log4j-api-2.24.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-api-2.24.3.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.apache.logging.log4j_log4j-api-2.24.3.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar at spark://10.100.1.243:39571/files/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.fasterxml.jackson.core_jackson-databind-2.15.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.module_jackson-module-scala_2.13-2.15.0.jar at spark://10.100.1.243:39571/files/com.fasterxml.jackson.module_jackson-module-scala_2.13-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.fasterxml.jackson.module_jackson-module-scala_2.13-2.15.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.fasterxml.jackson.module_jackson-module-scala_2.13-2.15.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar at spark://10.100.1.243:39571/files/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.fasterxml.jackson.core_jackson-annotations-2.15.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar at spark://10.100.1.243:39571/files/com.fasterxml.jackson.core_jackson-core-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.fasterxml.jackson.core_jackson-core-2.15.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.fasterxml.jackson.core_jackson-core-2.15.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar at spark://10.100.1.243:39571/files/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.fasterxml.jackson.dataformat_jackson-dataformat-xml-2.15.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.antlr_antlr4-runtime-4.13.1.jar at spark://10.100.1.243:39571/files/org.antlr_antlr4-runtime-4.13.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.antlr_antlr4-runtime-4.13.1.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.antlr_antlr4-runtime-4.13.1.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.antlr_antlr4-4.13.1.jar at spark://10.100.1.243:39571/files/org.antlr_antlr4-4.13.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.antlr_antlr4-4.13.1.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.antlr_antlr4-4.13.1.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar at spark://10.100.1.243:39571/files/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.apache.hadoop_hadoop-client-runtime-3.4.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar at spark://10.100.1.243:39571/files/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.fasterxml.jackson.datatype_jackson-datatype-jsr310-2.17.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar at spark://10.100.1.243:39571/files/org.openapitools_jackson-databind-nullable-0.2.6.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.openapitools_jackson-databind-nullable-0.2.6.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.openapitools_jackson-databind-nullable-0.2.6.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://10.100.1.243:39571/files/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.google.code.findbugs_jsr305-3.0.2.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-core-2.24.3.jar at spark://10.100.1.243:39571/files/org.apache.logging.log4j_log4j-core-2.24.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.apache.logging.log4j_log4j-core-2.24.3.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.apache.logging.log4j_log4j-core-2.24.3.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://10.100.1.243:39571/files/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.thoughtworks.paranamer_paranamer-2.8.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar at spark://10.100.1.243:39571/files/org.codehaus.woodstox_stax2-api-4.2.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.codehaus.woodstox_stax2-api-4.2.1.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.codehaus.woodstox_stax2-api-4.2.1.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar at spark://10.100.1.243:39571/files/com.fasterxml.woodstox_woodstox-core-6.5.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.fasterxml.woodstox_woodstox-core-6.5.1.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.fasterxml.woodstox_woodstox-core-6.5.1.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.antlr_antlr-runtime-3.5.3.jar at spark://10.100.1.243:39571/files/org.antlr_antlr-runtime-3.5.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.antlr_antlr-runtime-3.5.3.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.antlr_antlr-runtime-3.5.3.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.antlr_ST4-4.3.4.jar at spark://10.100.1.243:39571/files/org.antlr_ST4-4.3.4.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.antlr_ST4-4.3.4.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.antlr_ST4-4.3.4.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar at spark://10.100.1.243:39571/files/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/com.ibm.icu_icu4j-72.1.jar at spark://10.100.1.243:39571/files/com.ibm.icu_icu4j-72.1.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/com.ibm.icu_icu4j-72.1.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/com.ibm.icu_icu4j-72.1.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar at spark://10.100.1.243:39571/files/org.apache.hadoop_hadoop-client-api-3.4.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-api-3.4.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.apache.hadoop_hadoop-client-api-3.4.0.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar at spark://10.100.1.243:39571/files/org.xerial.snappy_snappy-java-1.1.10.4.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/org.xerial.snappy_snappy-java-1.1.10.4.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/org.xerial.snappy_snappy-java-1.1.10.4.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/commons-logging_commons-logging-1.2.jar at spark://10.100.1.243:39571/files/commons-logging_commons-logging-1.2.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/commons-logging_commons-logging-1.2.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/commons-logging_commons-logging-1.2.jar
26/01/12 18:59:42 INFO SparkContext: Added file file:///root/.ivy2.5.2/jars/io.delta_delta-storage-4.0.0.jar at spark://10.100.1.243:39571/files/io.delta_delta-storage-4.0.0.jar with timestamp 1768244381329
26/01/12 18:59:42 INFO Utils: Copying /root/.ivy2.5.2/jars/io.delta_delta-storage-4.0.0.jar to /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/userFiles-c863165d-10f9-40a3-81bb-1ae154165d8f/io.delta_delta-storage-4.0.0.jar
26/01/12 18:59:42 INFO SecurityManager: Changing view acls to: root
26/01/12 18:59:42 INFO SecurityManager: Changing modify acls to: root
26/01/12 18:59:42 INFO SecurityManager: Changing view acls groups to: root
26/01/12 18:59:42 INFO SecurityManager: Changing modify acls groups to: root
26/01/12 18:59:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
26/01/12 18:59:42 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
26/01/12 18:59:44 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
26/01/12 18:59:44 INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors,spark.dynamicAllocation.minExecutors and spark.executor.instances
26/01/12 18:59:44 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1, known: 0, sharedSlotFromPendingPods: 2147483647.
26/01/12 18:59:44 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
26/01/12 18:59:44 INFO BasicExecutorFeatureStep: Adding decommission script to lifecycle
26/01/12 18:59:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45745.
26/01/12 18:59:44 INFO NettyBlockTransferService: Server created on 10.100.1.243:45745
26/01/12 18:59:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/01/12 18:59:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.100.1.243, 45745, None)
26/01/12 18:59:44 INFO BlockManagerMasterEndpoint: Registering block manager 10.100.1.243:45745 with 1048.8 MiB RAM, BlockManagerId(driver, 10.100.1.243, 45745, None)
26/01/12 18:59:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.100.1.243, 45745, None)
26/01/12 18:59:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.100.1.243, 45745, None)
26/01/12 18:59:44 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.
26/01/12 18:59:44 INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors,spark.dynamicAllocation.minExecutors and spark.executor.instances
26/01/12 18:59:44 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.
26/01/12 18:59:44 WARN ExecutorAllocationManager: You are enabling both shuffle tracking and other DA supported mechanism, which will cause idle executors not to be released in a timely, please check the configurations.
26/01/12 19:00:14 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000.0(ms)
Using Catalog: unity
Creating Table unity.default.fixed_table...
26/01/12 19:00:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
26/01/12 19:00:18 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
26/01/12 19:00:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/01/12 19:00:21 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/01/12 19:00:21 INFO MetricsSystemImpl: s3a-file-system metrics system started
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
26/01/12 19:00:22 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3a
26/01/12 19:00:23 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
26/01/12 19:00:23 INFO DummySnapshot: [tableId=f73652e8-0cbc-46f9-bf14-0c28f10e3518] Created snapshot DummySnapshot(path=s3a://test-bucket/fixed_table/_delta_log, version=-1, metadata=Metadata(088eba92-14b2-4529-9658-5f1483e83241,null,null,Format(parquet,Map()),null,List(),Map(),Some(1768244423162)), logSegment=LogSegment(s3a://test-bucket/fixed_table/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@69458cd7,-1), checksumOpt=None)
26/01/12 19:00:23 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
26/01/12 19:00:23 INFO DummySnapshot: [tableId=088eba92-14b2-4529-9658-5f1483e83241] Created snapshot DummySnapshot(path=s3a://test-bucket/fixed_table/_delta_log, version=-1, metadata=Metadata(ce5a4b0a-7d34-4869-877e-2bf7ebf93b3d,null,null,Format(parquet,Map()),null,List(),Map(),Some(1768244423337)), logSegment=LogSegment(s3a://test-bucket/fixed_table/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@69458cd7,-1), checksumOpt=None)
26/01/12 19:00:23 INFO IcebergCompatV2: [tableId=ce5a4b0a-7d34-4869-877e-2bf7ebf93b3d] IcebergCompatV1 auto-supporting table features: HashSet(columnMapping)
26/01/12 19:00:23 INFO IcebergCompatV2: [tableId=ce5a4b0a-7d34-4869-877e-2bf7ebf93b3d] IcebergCompatV1 auto-setting table properties: HashMap(delta.columnMapping.mode -> name)
26/01/12 19:00:23 INFO OptimisticTransaction: [tableId=ce5a4b0a,txnId=fe678df9] Updated metadata from - to Metadata(3c5a0e06-5662-4a02-a687-3357dfc50f7e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"delta.columnMapping.id":1,"delta.columnMapping.physicalName":"col-0035aed3-936a-48ef-a656-f7a5fe1c6fbf","delta.columnMapping.nested.ids":{}}},{"name":"data","type":"string","nullable":true,"metadata":{"delta.columnMapping.id":2,"delta.columnMapping.physicalName":"col-de324869-67aa-4b39-abf5-b5b8c1bdbc8d","delta.columnMapping.nested.ids":{}}},{"name":"timestamp","type":"timestamp","nullable":true,"metadata":{"delta.columnMapping.id":3,"delta.columnMapping.physicalName":"col-eae04dc7-3ef3-4b74-82fa-8d23b7f89d4a","delta.columnMapping.nested.ids":{}}}]},List(),Map(delta.enableIcebergCompatV2 -> true, delta.universalFormat.enabledFormats -> iceberg, delta.columnMapping.mode -> name, delta.columnMapping.maxColumnId -> 3),Some(1768244423444))
26/01/12 19:00:23 INFO OptimisticTransaction: [tableId=ce5a4b0a,txnId=fe678df9] Attempting to commit version 0 with 3 actions with Serializable isolation level
26/01/12 19:00:24 INFO OptimisticTransaction: Incremental commit: starting with snapshot version -1
26/01/12 19:00:26 INFO CodeGenerator: Code generated in 555.85017 ms
26/01/12 19:00:26 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
26/01/12 19:00:26 INFO DAGScheduler: Job 0 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 9.532204 ms
26/01/12 19:00:26 INFO DeltaLog: Creating a new snapshot v0 for commit version 0
26/01/12 19:00:26 INFO DeltaLog: Loading version 0.
26/01/12 19:00:26 INFO Snapshot: [tableId=ce5a4b0a-7d34-4869-877e-2bf7ebf93b3d] Created snapshot Snapshot(path=s3a://test-bucket/fixed_table/_delta_log, version=0, metadata=Metadata(3c5a0e06-5662-4a02-a687-3357dfc50f7e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"delta.columnMapping.id":1,"delta.columnMapping.physicalName":"col-0035aed3-936a-48ef-a656-f7a5fe1c6fbf","delta.columnMapping.nested.ids":{}}},{"name":"data","type":"string","nullable":true,"metadata":{"delta.columnMapping.id":2,"delta.columnMapping.physicalName":"col-de324869-67aa-4b39-abf5-b5b8c1bdbc8d","delta.columnMapping.nested.ids":{}}},{"name":"timestamp","type":"timestamp","nullable":true,"metadata":{"delta.columnMapping.id":3,"delta.columnMapping.physicalName":"col-eae04dc7-3ef3-4b74-82fa-8d23b7f89d4a","delta.columnMapping.nested.ids":{}}}]},List(),Map(delta.enableIcebergCompatV2 -> true, delta.universalFormat.enabledFormats -> iceberg, delta.columnMapping.mode -> name, delta.columnMapping.maxColumnId -> 3),Some(1768244423444)), logSegment=LogSegment(s3a://test-bucket/fixed_table/_delta_log,0,List(S3AFileStatus{path=s3a://test-bucket/fixed_table/_delta_log/00000000000000000000.json; isDirectory=false; length=1724; replication=1; blocksize=33554432; modification_time=1768244424000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag="da052a786eacb036eb9acf1cbb422a08" versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@69458cd7,1768244424000), checksumOpt=Some(VersionChecksum(Some(fe678df9-20a7-49ef-8b3e-33c4825ed23c),0,0,None,None,1,1,None,Some(List()),Some(List()),Metadata(3c5a0e06-5662-4a02-a687-3357dfc50f7e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"delta.columnMapping.id":1,"delta.columnMapping.physicalName":"col-0035aed3-936a-48ef-a656-f7a5fe1c6fbf","delta.columnMapping.nested.ids":{}}},{"name":"data","type":"string","nullable":true,"metadata":{"delta.columnMapping.id":2,"delta.columnMapping.physicalName":"col-de324869-67aa-4b39-abf5-b5b8c1bdbc8d","delta.columnMapping.nested.ids":{}}},{"name":"timestamp","type":"timestamp","nullable":true,"metadata":{"delta.columnMapping.id":3,"delta.columnMapping.physicalName":"col-eae04dc7-3ef3-4b74-82fa-8d23b7f89d4a","delta.columnMapping.nested.ids":{}}}]},List(),Map(delta.enableIcebergCompatV2 -> true, delta.universalFormat.enabledFormats -> iceberg, delta.columnMapping.mode -> name, delta.columnMapping.maxColumnId -> 3),Some(1768244423444)),Protocol(2,7,None,[appendOnly,columnMapping,icebergCompatV2,invariants]),None,None,Some(List()))))
26/01/12 19:00:26 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://test-bucket/fixed_table/_delta_log, version=0, metadata=Metadata(3c5a0e06-5662-4a02-a687-3357dfc50f7e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"delta.columnMapping.id":1,"delta.columnMapping.physicalName":"col-0035aed3-936a-48ef-a656-f7a5fe1c6fbf","delta.columnMapping.nested.ids":{}}},{"name":"data","type":"string","nullable":true,"metadata":{"delta.columnMapping.id":2,"delta.columnMapping.physicalName":"col-de324869-67aa-4b39-abf5-b5b8c1bdbc8d","delta.columnMapping.nested.ids":{}}},{"name":"timestamp","type":"timestamp","nullable":true,"metadata":{"delta.columnMapping.id":3,"delta.columnMapping.physicalName":"col-eae04dc7-3ef3-4b74-82fa-8d23b7f89d4a","delta.columnMapping.nested.ids":{}}}]},List(),Map(delta.enableIcebergCompatV2 -> true, delta.universalFormat.enabledFormats -> iceberg, delta.columnMapping.mode -> name, delta.columnMapping.maxColumnId -> 3),Some(1768244423444)), logSegment=LogSegment(s3a://test-bucket/fixed_table/_delta_log,0,List(S3AFileStatus{path=s3a://test-bucket/fixed_table/_delta_log/00000000000000000000.json; isDirectory=false; length=1724; replication=1; blocksize=33554432; modification_time=1768244424000; access_time=0; owner=root; group=root; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag="da052a786eacb036eb9acf1cbb422a08" versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@69458cd7,1768244424000), checksumOpt=Some(VersionChecksum(Some(fe678df9-20a7-49ef-8b3e-33c4825ed23c),0,0,None,None,1,1,None,Some(List()),Some(List()),Metadata(3c5a0e06-5662-4a02-a687-3357dfc50f7e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{"delta.columnMapping.id":1,"delta.columnMapping.physicalName":"col-0035aed3-936a-48ef-a656-f7a5fe1c6fbf","delta.columnMapping.nested.ids":{}}},{"name":"data","type":"string","nullable":true,"metadata":{"delta.columnMapping.id":2,"delta.columnMapping.physicalName":"col-de324869-67aa-4b39-abf5-b5b8c1bdbc8d","delta.columnMapping.nested.ids":{}}},{"name":"timestamp","type":"timestamp","nullable":true,"metadata":{"delta.columnMapping.id":3,"delta.columnMapping.physicalName":"col-eae04dc7-3ef3-4b74-82fa-8d23b7f89d4a","delta.columnMapping.nested.ids":{}}}]},List(),Map(delta.enableIcebergCompatV2 -> true, delta.universalFormat.enabledFormats -> iceberg, delta.columnMapping.mode -> name, delta.columnMapping.maxColumnId -> 3),Some(1768244423444)),Protocol(2,7,None,[appendOnly,columnMapping,icebergCompatV2,invariants]),None,None,Some(List()))))
26/01/12 19:00:26 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 1724)
26/01/12 19:00:26 INFO OptimisticTransaction: [tableId=ce5a4b0a,txnId=fe678df9] Committed delta #0 to s3a://test-bucket/fixed_table/_delta_log
26/01/12 19:00:26 INFO ChecksumHook: Writing checksum file for table path s3a://test-bucket/fixed_table/_delta_log version 0
26/01/12 19:00:27 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
26/01/12 19:00:27 INFO CheckpointFileManager: Writing atomically to s3a://test-bucket/fixed_table/_delta_log/00000000000000000000.crc using temp file s3a://test-bucket/fixed_table/_delta_log/.00000000000000000000.crc.b0ff08f1-cdd5-449a-93e9-e17b860a198c.tmp
26/01/12 19:00:27 INFO CheckpointFileManager: Renamed temp file s3a://test-bucket/fixed_table/_delta_log/.00000000000000000000.crc.b0ff08f1-cdd5-449a-93e9-e17b860a198c.tmp to s3a://test-bucket/fixed_table/_delta_log/00000000000000000000.crc
26/01/12 19:00:27 INFO CreateDeltaTableCommand: Table is path-based table: false. Update catalog with mode: Create
Traceback (most recent call last):
  File "/opt/scripts/..2026_01_12_18_57_51.3733404476/lakehouse_verification.py", line 22, in <module>
    spark.sql(f"""
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1810, in sql
  File "/opt/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 282, in deco
  File "/opt/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o83.sql.
: io.unitycatalog.client.ApiException: createTable call failed with: 400 - {"error_code":"INVALID_ARGUMENT","details":[{"reason":"INVALID_ARGUMENT","metadata":{},"@type":"google.rpc.ErrorInfo"}],"stack_trace":null,"message":"Unsupported URI scheme: s3a://test-bucket/fixed_table"}
	at io.unitycatalog.client.api.TablesApi.getApiException(TablesApi.java:78)
	at io.unitycatalog.client.api.TablesApi.createTableWithHttpInfo(TablesApi.java:192)
	at io.unitycatalog.client.api.TablesApi.createTable(TablesApi.java:170)
	at io.unitycatalog.spark.UCProxy.createTable(UCSingleCatalog.scala:435)
	at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:230)
	at io.unitycatalog.spark.UCProxy.createTable(UCSingleCatalog.scala:279)
	at org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createTable(DelegatingCatalogExtension.java:111)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.super$createTable(DeltaCatalog.scala:221)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$14(DeltaCatalog.scala:221)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$14$adapted(DeltaCatalog.scala:219)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableLike.updateCatalog(CreateDeltaTableLike.scala:98)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableLike.updateCatalog$(CreateDeltaTableLike.scala:85)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:68)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.runPostCommitUpdates(CreateDeltaTableCommand.scala:242)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:212)
	at org.apache.spark.sql.delta.OptimisticTransaction$.withActive(OptimisticTransaction.scala:213)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:176)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$4(CreateDeltaTableCommand.scala:141)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:68)
	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:139)
	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:68)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:138)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:128)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:118)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:68)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:140)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:223)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:69)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:105)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createTable$1(DeltaCatalog.scala:372)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:69)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:352)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:343)
	at io.unitycatalog.spark.UCSingleCatalog.createTable(UCSingleCatalog.scala:185)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:46)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:277)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:140)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)
	at org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)
	at org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at io.unitycatalog.client.api.TablesApi.getApiException(TablesApi.java:78)
		at io.unitycatalog.client.api.TablesApi.createTableWithHttpInfo(TablesApi.java:192)
		at io.unitycatalog.client.api.TablesApi.createTable(TablesApi.java:170)
		at io.unitycatalog.spark.UCProxy.createTable(UCSingleCatalog.scala:435)
		at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:230)
		at io.unitycatalog.spark.UCProxy.createTable(UCSingleCatalog.scala:279)
		at org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createTable(DelegatingCatalogExtension.java:111)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.super$createTable(DeltaCatalog.scala:221)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$14(DeltaCatalog.scala:221)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$14$adapted(DeltaCatalog.scala:219)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableLike.updateCatalog(CreateDeltaTableLike.scala:98)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableLike.updateCatalog$(CreateDeltaTableLike.scala:85)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:68)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.runPostCommitUpdates(CreateDeltaTableCommand.scala:242)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:212)
		at org.apache.spark.sql.delta.OptimisticTransaction$.withActive(OptimisticTransaction.scala:213)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:176)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.$anonfun$run$4(CreateDeltaTableCommand.scala:141)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:68)
		at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:139)
		at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
		at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:68)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:138)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:128)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:118)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:68)
		at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:140)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:223)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:69)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:105)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createTable$1(DeltaCatalog.scala:372)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)
		at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:69)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:352)
		at org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:343)
		at io.unitycatalog.spark.UCSingleCatalog.createTable(UCSingleCatalog.scala:185)
		at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:46)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 22 more

26/01/12 19:00:27 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at SparkSubmit.scala:1036.
26/01/12 19:00:27 INFO SparkUI: Stopped Spark web UI at http://10.100.1.243:4040
26/01/12 19:00:27 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
26/01/12 19:00:27 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
26/01/12 19:00:27 INFO ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.
26/01/12 19:00:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
26/01/12 19:00:28 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
26/01/12 19:00:28 INFO MemoryStore: MemoryStore cleared
26/01/12 19:00:28 INFO BlockManager: BlockManager stopped
26/01/12 19:00:28 INFO BlockManagerMaster: BlockManagerMaster stopped
26/01/12 19:00:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
26/01/12 19:00:28 INFO SparkContext: Successfully stopped SparkContext
26/01/12 19:00:28 INFO ShutdownHookManager: Shutdown hook called
26/01/12 19:00:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f/pyspark-4ca9ff90-c319-44d4-aca2-c34f960b4cdd
26/01/12 19:00:28 INFO ShutdownHookManager: Deleting directory /tmp/artifacts-b162485b-19e0-4ea0-83cc-17ec759fc175
26/01/12 19:00:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-f999d7c8-a401-4be0-b065-8d25e0e6360f
26/01/12 19:00:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-c14937fe-a7b1-4c29-953a-eb36f8ee7738
26/01/12 19:00:30 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
26/01/12 19:00:30 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
26/01/12 19:00:30 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
